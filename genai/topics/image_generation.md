# Image Generation

## Key Concepts
- Image generation uses generative models to create new images from text, noise, or other images.
- Key models: GANs, diffusion models, transformers (DALL-E, Stable Diffusion).
- Applications: art, design, data augmentation, entertainment.

## Real-World Examples

### Example 1: DALL-E
- Generates original images from natural language prompts.

### Example 2: Stable Diffusion
- Produces photorealistic and artistic images from text.

### Example 3: Style Transfer
- AI applies the style of one image to another (e.g., Prisma app).

## Interview Questions & Answers

**Q1: How do text-to-image models work?**
A: They map text prompts to image features using deep learning, often with transformers or diffusion models.

**Q2: Real-time: How do you evaluate the quality of generated images?**
A: Use metrics like FID (Fr√©chet Inception Distance), human evaluation, and diversity checks.

**Q3: What are common challenges in image generation?**
A: Artifacts, bias, lack of diversity, and copyright concerns.

## References
- [DALL-E (OpenAI)](https://openai.com/research/publications/dall-e)
- [Stable Diffusion (Stability AI)](https://stability.ai/blog/stable-diffusion-public-release)

![DALL-E Example](https://openai.com/research/publications/dall-e/dall-e-fig1.png)
*Image Source: OpenAI, DALL-E*
